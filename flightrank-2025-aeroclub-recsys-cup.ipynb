{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":12202936,"sourceType":"datasetVersion","datasetId":7686838},{"sourceId":456749,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":370374,"modelId":391270}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install duckdb lightgbm optuna category_encoders","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:19.518686Z","iopub.execute_input":"2025-07-03T12:48:19.518897Z","iopub.status.idle":"2025-07-03T12:48:33.852832Z","shell.execute_reply.started":"2025-07-03T12:48:19.518874Z","shell.execute_reply":"2025-07-03T12:48:33.846825Z"}},"outputs":[{"name":"stdout","text":"Collecting duckdb\n  Downloading duckdb-1.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (21.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting lightgbm\n  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting optuna\n  Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting category_encoders\n  Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/site-packages (from lightgbm) (2.0.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/site-packages (from lightgbm) (1.15.2)\nCollecting colorlog\n  Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/site-packages (from optuna) (6.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from optuna) (4.67.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/site-packages (from optuna) (25.0)\nCollecting sqlalchemy>=1.4.2\n  Downloading sqlalchemy-2.0.41-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting alembic>=1.5.0\n  Downloading alembic-1.16.2-py3-none-any.whl (242 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting statsmodels>=0.9.0\n  Downloading statsmodels-0.14.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting patsy>=0.5.1\n  Downloading patsy-1.0.1-py2.py3-none-any.whl (232 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.9/232.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.10/site-packages (from category_encoders) (1.6.1)\nRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/site-packages (from category_encoders) (2.2.3)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (4.13.2)\nCollecting Mako\n  Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tomli in /usr/local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (2.2.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2025.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\nCollecting greenlet>=1\n  Downloading greenlet-3.2.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (582 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m582.2/582.2 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\nInstalling collected packages: patsy, Mako, greenlet, duckdb, colorlog, sqlalchemy, lightgbm, statsmodels, alembic, optuna, category_encoders\nSuccessfully installed Mako-1.3.10 alembic-1.16.2 category_encoders-2.8.1 colorlog-6.9.0 duckdb-1.3.1 greenlet-3.2.3 lightgbm-4.6.0 optuna-4.4.0 patsy-1.0.1 sqlalchemy-2.0.41 statsmodels-0.14.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gc\ndel test_path\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:48:12.125692Z","iopub.execute_input":"2025-07-03T10:48:12.126055Z","iopub.status.idle":"2025-07-03T10:48:15.946439Z","shell.execute_reply.started":"2025-07-03T10:48:12.126025Z","shell.execute_reply":"2025-07-03T10:48:15.942169Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1540"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ===========================\n# Load and Sample Training Data (DuckDB)\n# ===========================\n# This section loads the AeroClub RecSys Cup 2025 training dataset using DuckDB for efficient querying.\n# \n# ðŸ”¹ We filter sessions (`ranker_id`) that have at least 10 flight options to ensure meaningful ranking.\n# ðŸ”¹ From these, we randomly sample 10,000 sessions for faster experimentation (adjust as needed for memory).\n# ðŸ”¹ The query joins back to get full records for the sampled sessions.\n# ðŸ”¹ DuckDB is preferred here for fast, SQL-like filtering on large Parquet files without loading everything into memory.\n#\n# ðŸ‘‰ You can disable sampling for full dataset access by switching to the fallback query below.\n# # (Optional) Load entire dataset without filtering or sampling\n# query = f\"\"\"\n#     SELECT *\n#     FROM parquet_scan('{train_path}')\n# \"\"\"\n\n\nimport duckdb\nimport pandas as pd\n\ntrain_path = \"/kaggle/input/aeroclub-recsys-cup-2025-1/train.parquet\"\ntest_path = \"/kaggle/input/aeroclub-recsys-cup-2025-1/test.parquet\"\n\ncon = duckdb.connect()\n\n# Correct session sampling with minimum flight options\nquery = f\"\"\"\n    WITH valid_sessions AS (\n        SELECT ranker_id\n        FROM parquet_scan('{train_path}')\n        GROUP BY ranker_id\n        HAVING COUNT(*) >= 10  -- Sessions with at least 10 flight options\n    ),\n    sampled_sessions AS (\n        SELECT ranker_id\n        FROM valid_sessions\n        USING SAMPLE 10000  -- Sample 5000 sessions (adjust based on memory)\n    )\n    SELECT t.*\n    FROM parquet_scan('{train_path}') t\n    JOIN sampled_sessions s ON t.ranker_id = s.ranker_id\n\"\"\"\n\n# query = f\"\"\"\n#     SELECT *\n#     FROM parquet_scan('{train_path}')\n# \"\"\"\n\n# Execute query and convert to DataFrame\ndf = con.query(query).to_df()\ncon.close()\n\n# Check session stats\nsession_counts = df['ranker_id'].value_counts()\nprint(f\"Loaded {len(df)} rows from {len(session_counts)} sessions\")\nprint(f\"Min options per session: {session_counts.min()}\")\nprint(f\"Max options per session: {session_counts.max()}\")\nprint(f\"Selected flights: {df['selected'].sum()} ({(df['selected'].mean()*100):.2f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:48:48.667479Z","iopub.execute_input":"2025-07-03T12:48:48.667798Z","iopub.status.idle":"2025-07-03T12:49:06.666184Z","shell.execute_reply.started":"2025-07-03T12:48:48.667770Z","shell.execute_reply":"2025-07-03T12:49:06.660476Z"}},"outputs":[{"name":"stdout","text":"Loaded 2007739 rows from 10000 sessions\nMin options per session: 10\nMax options per session: 7841\nSelected flights: 10000 (0.50%)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ===========================\n# Pandas Display Settings\n# ===========================\n# These options improve the readability of large DataFrames during development.\n#\n# ðŸ”¹ display.max_rows:    Show up to 500 rows (instead of truncating after 10).\n# ðŸ”¹ display.max_columns: Show up to 500 columns (helps when working with wide datasets).\n# ðŸ”¹ display.width:       Sets the display width to 1000 characters so rows donâ€™t wrap across lines.\n#\n# ðŸ‘‰ These settings are useful for debugging and exploration in notebooks or logs.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:49:06.668549Z","iopub.execute_input":"2025-07-03T12:49:06.668953Z","iopub.status.idle":"2025-07-03T12:49:06.678564Z","shell.execute_reply.started":"2025-07-03T12:49:06.668914Z","shell.execute_reply":"2025-07-03T12:49:06.673950Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ===========================\n# Missing Value Imputation\n# ===========================\n# We begin by copying the original DataFrame to preserve raw data.\n# Then, we report the total number of missing values before imputation.\n\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\n\ndf_filled = df.copy()\n\nprint(\"Starting missing value imputation...\")\nprint(f\"Original missing values: {df.isnull().sum().sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T12:49:06.680770Z","iopub.execute_input":"2025-07-03T12:49:06.681020Z","iopub.status.idle":"2025-07-03T12:49:13.404691Z","shell.execute_reply.started":"2025-07-03T12:49:06.680996Z","shell.execute_reply":"2025-07-03T12:49:13.398594Z"}},"outputs":[{"name":"stdout","text":"Starting missing value imputation...\nOriginal missing values: 149332775\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# def preprocess_flight_data(df):\n#     df = df.copy()\n    \n#     # ========================\n#     # 1. Column Renaming\n#     # ========================\n#     rename_dict = {\n#         'Id': 'id',\n#         'ranker_id': 'ranker_id',\n#         'profileId': 'profile_id',\n#         'companyID': 'company_id',\n#         'sex': 'user_gender',\n#         'nationality': 'user_nationality',\n#         'frequentFlyer': 'frequent_flyer_status',\n#         'isVip': 'is_vip',\n#         'bySelf': 'is_self_booking',\n#         'isAccess3D': 'is_access_3d',\n#         'corporateTariffCode': 'corporate_tariff_code',\n#         'searchRoute': 'search_route',\n#         'requestDate': 'request_datetime',\n#         'totalPrice': 'total_price',\n#         'taxes': 'taxes_amount',\n#         'pricingInfo_isAccessTP': 'pricing_policy_compliance',\n#         'pricingInfo_passengerCount': 'passenger_count',\n#         'selected': 'selected'\n#     }\n#     df.rename(columns=rename_dict, inplace=True)\n    \n#     # ========================\n#     # 2. Feature Engineering\n#     # ========================\n    \n#     # Trip type from search route\n#     df['is_round_trip'] = df['search_route'].str.contains('/').astype(int)\n#     print('is_roundtrip')\n    \n#     # Convert datetimes - handle \"NONE\" values\n#     df['request_datetime'] = pd.to_datetime(df['request_datetime'], errors='coerce')\n#     print('requestdatetime')\n    \n#     # Process legs\n#     for leg in [0, 1]:\n#         leg_prefix = f'legs{leg}_'\n        \n#         # Skip leg1 if it doesn't exist\n#         if f'{leg_prefix}departureAt' not in df.columns:\n#             continue\n            \n#         # Convert duration to numeric and calculate hours\n#         dur_col = f'{leg_prefix}duration'\n#         df[dur_col] = pd.to_numeric(df[dur_col], errors='coerce')\n#         df[f'legs{leg}_duration_hours'] = df[dur_col] / 60  # Keep original prefix\n        \n#         # Datetime features - handle \"NONE\" values\n#         for time_type in ['departureAt', 'arrivalAt']:\n#             col_name = f'{leg_prefix}{time_type}'\n#             # Replace \"NONE\" with NaT (Not a Time)\n#             df[col_name] = df[col_name].replace('NONE', pd.NaT)\n#             # Convert to datetime with error handling\n#             df[col_name] = pd.to_datetime(df[col_name], errors='coerce')\n#             # Create time features without renaming the original column\n#             df[f'{leg_prefix}{time_type}_hour'] = df[col_name].dt.hour\n#             df[f'{leg_prefix}{time_type}_day_of_week'] = df[col_name].dt.dayofweek\n        \n#         # Segment processing\n#         for seg in range(4):  # 0-3 segments\n#             seg_prefix = f'{leg_prefix}segments{seg}_'\n            \n#             # Skip if segment doesn't exist\n#             if f'{seg_prefix}departureFrom_airport_iata' not in df.columns:\n#                 continue\n                \n#             # Airport features\n#             df[f'{leg_prefix}seg{seg}_departure_airport'] = df[f'{seg_prefix}departureFrom_airport_iata']\n#             df[f'{leg_prefix}seg{seg}_arrival_airport'] = df[f'{seg_prefix}arrivalTo_airport_iata']\n#             df[f'{leg_prefix}seg{seg}_arrival_city'] = df[f'{seg_prefix}arrivalTo_airport_city_iata']\n            \n#             # Airline features\n#             marketing = df[f'{seg_prefix}marketingCarrier_code'].astype(str)\n#             operating = df[f'{seg_prefix}operatingCarrier_code'].astype(str)\n#             df[f'{leg_prefix}seg{seg}_marketing_carrier'] = marketing\n#             df[f'{leg_prefix}seg{seg}_operating_carrier'] = operating\n#             df[f'{leg_prefix}seg{seg}_carrier_same'] = (marketing == operating).astype(int)\n            \n#             # Flight details\n#             df[f'{leg_prefix}seg{seg}_aircraft_type'] = df[f'{seg_prefix}aircraft_code']\n#             df[f'{leg_prefix}seg{seg}_flight_number'] = df[f'{seg_prefix}flightNumber']\n            \n#             # Convert segment duration to numeric\n#             seg_dur_col = f'{seg_prefix}duration'\n#             df[seg_dur_col] = pd.to_numeric(df[seg_dur_col], errors='coerce')\n#             df[f'{leg_prefix}seg{seg}_duration_hours'] = df[seg_dur_col] / 60\n            \n#             df[f'{leg_prefix}seg{seg}_seats_available'] = df[f'{seg_prefix}seatsAvailable']\n            \n#             # Baggage features\n#             baggage_col = f'{seg_prefix}baggageAllowance_quantity'\n#             df[baggage_col] = pd.to_numeric(df[baggage_col], errors='coerce')\n#             df[f'{leg_prefix}seg{seg}_baggage_allowance'] = df[baggage_col]\n#             df[f'{leg_prefix}seg{seg}_baggage_type'] = df[f'{seg_prefix}baggageAllowance_weightMeasurementType']\n            \n#             # Cabin class (mapped to text)\n#             cabin_map = {1.0: 'economy', 2.0: 'business', 4.0: 'premium'}\n#             df[f'{leg_prefix}seg{seg}_cabin_class'] = df[f'{seg_prefix}cabinClass'].map(cabin_map)\n\n#         print('processed leg')\n        \n#         # Aggregate leg-level features\n#         # Cabin class summary (best cabin in leg)\n#         cabin_priority = {'economy': 0, 'premium': 1, 'business': 2}\n#         for i in range(4):\n#             cabin_col = f'{leg_prefix}seg{i}_cabin_class'\n#             if cabin_col in df.columns:\n#                 df[f'{leg_prefix}seg{i}_cabin_priority'] = df[cabin_col].map(cabin_priority)\n        \n#         cabin_cols = [f'{leg_prefix}seg{i}_cabin_priority' for i in range(4) \n#                      if f'{leg_prefix}seg{i}_cabin_priority' in df.columns]\n#         if cabin_cols:\n#             df[f'{leg_prefix}best_cabin'] = df[cabin_cols].max(axis=1).map({v:k for k,v in cabin_priority.items()})\n        \n#         # Total baggage allowance\n#         baggage_cols = [f'{leg_prefix}seg{i}_baggage_allowance' for i in range(4)\n#                        if f'{leg_prefix}seg{i}_baggage_allowance' in df.columns]\n#         if baggage_cols:\n#             df[f'{leg_prefix}total_baggage'] = df[baggage_cols].sum(axis=1)\n\n#         print('processed baggage and cabin')\n    \n#     # Process rules\n#     rule_types = {0: 'cancellation', 1: 'exchange'}\n#     for rule_num, rule_name in rule_types.items():\n#         prefix = f'miniRules{rule_num}_'\n#         df[f'{rule_name}_penalty_amount'] = pd.to_numeric(df[f'{prefix}monetaryAmount'], errors='coerce')\n#         df[f'{rule_name}_penalty_percentage'] = pd.to_numeric(df[f'{prefix}percentage'], errors='coerce')\n#         df[f'{rule_name}_allowed'] = (pd.to_numeric(df[f'{prefix}statusInfos'], errors='coerce') != 0).astype(int)\n\n#     print('processed rule type')\n    \n#     # Pricing features\n#     df['base_fare'] = df['total_price'] - df['taxes_amount']\n#     df['price_per_passenger'] = df['total_price'] / df['passenger_count'].clip(lower=1)\n#     print('processed pricing feature')\n    \n#     # Time until departure - FIXED: use original column name\n#     if 'legs0_departureAt' in df.columns:\n#         df['days_until_departure'] = (df['legs0_departureAt'] - df['request_datetime']).dt.days\n#     print('processed time until departure')\n#     # ========================\n#     # 3. Column Removal\n#     # ========================\n    \n#     # Remove original nested columns\n#     cols_to_remove = []\n    \n#     # Remove original leg columns we don't need anymore\n#     cols_to_remove.extend([\n#         'legs0_duration', 'legs1_duration',\n#         'search_route'  # We extracted is_round_trip\n#     ])\n    \n#     # Remove all original segment columns\n#     for col in df.columns:\n#         if col.startswith('legs') and any(part in col for part in ['segments', 'marketingCarrier', 'operatingCarrier', \n#                                                                 'aircraft_code', 'flightNumber', 'duration', \n#                                                                 'baggageAllowance', 'cabinClass']):\n#             cols_to_remove.append(col)\n    \n#     # Remove miniRules columns\n#     cols_to_remove.extend([f'miniRules{i}_{part}' for i in [0,1] \n#                           for part in ['monetaryAmount', 'percentage', 'statusInfos']])\n    \n#     # Remove columns that don't exist in the DataFrame\n#     cols_to_remove = [col for col in cols_to_remove if col in df.columns]\n    \n#     # Drop the columns\n#     df.drop(columns=cols_to_remove, inplace=True, errors='ignore')\n    \n#     # ========================\n#     # 4. Final Processing\n#     # ========================\n    \n#     # Convert categorical columns\n#     categorical_cols = [\n#         'user_gender', 'user_nationality', 'frequent_flyer_status',\n#         'corporate_tariff_code'\n#     ]\n#     # Only convert existing columns\n#     categorical_cols = [col for col in categorical_cols if col in df.columns]\n#     for col in categorical_cols:\n#         df[col] = df[col].astype('category')\n    \n#     # Fill missing values for one-way trips\n#     if 'legs1_departureAt' in df.columns:\n#         leg1_cols = [col for col in df.columns if col.startswith('legs1')]\n#         # Create a dictionary for fill values\n#         fill_dict = {\n#             'legs1_duration_hours': 0,\n#             'legs1_total_baggage': 0,\n#             'legs1_best_cabin': 'none'\n#         }\n#         # Only include columns that actually exist in the DataFrame\n#         fill_dict = {k: v for k, v in fill_dict.items() if k in df.columns}\n#         df[leg1_cols] = df[leg1_cols].fillna(fill_dict)\n    \n#     # Fill other NaNs with appropriate defaults\n#     numeric_cols = df.select_dtypes(include=np.number).columns\n#     df[numeric_cols] = df[numeric_cols].fillna(0)\n    \n#     # Fill datetime NaNs with a default date (e.g., 1970-01-01) or keep as NaT\n#     datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns\n#     # Option: Keep as NaT or fill with a specific date\n#     # df[datetime_cols] = df[datetime_cols].fillna(pd.Timestamp('1970-01-01'))\n    \n#     # Fill object columns with 'missing'\n#     object_cols = df.select_dtypes(include='object').columns\n#     df[object_cols] = df[object_cols].fillna('missing')\n    \n#     return df\n\n# # Load your dataset\n# # df = pd.read_csv('your_data.csv')\n\n# # Apply preprocessing\n# processed_df = preprocess_flight_data(df_filled)\n\n# # Check for remaining type issues\n# print(processed_df.dtypes)\n\n# # Save processed data\n# # processed_df.to_csv('processed_flight_data.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:30:01.798700Z","iopub.execute_input":"2025-07-03T13:30:01.798996Z","iopub.status.idle":"2025-07-03T13:30:01.814668Z","shell.execute_reply.started":"2025-07-03T13:30:01.798970Z","shell.execute_reply":"2025-07-03T13:30:01.810399Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Corporate Tariff Code (Int64 dtype) - Fill with pandas NA for integer columns\n# Int64 is pandas nullable integer type, so we use pd.NA instead of string\n# Convert to regular int64 and use -1 as indicator for \"no corporate tariff\"\ndf_filled['corporateTariffCode'] = df_filled['corporateTariffCode'].fillna(-1).astype('int64')\nprint(\"âœ“ Filled corporateTariffCode with -1 (converted to int64)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:29:17.469666Z","iopub.execute_input":"2025-07-03T13:29:17.469973Z","iopub.status.idle":"2025-07-03T13:29:17.478301Z","shell.execute_reply.started":"2025-07-03T13:29:17.469948Z","shell.execute_reply":"2025-07-03T13:29:17.474907Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Travel Policy Access (float64) - fill with 0.0 (non-compliant/not applicable)\ndf_filled['pricingInfo_isAccessTP'].fillna(0.0, inplace=True)\nprint(\"âœ“ Filled pricingInfo_isAccessTP with 0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:51:33.196668Z","iopub.execute_input":"2025-07-03T10:51:33.196960Z","iopub.status.idle":"2025-07-03T10:51:33.299638Z","shell.execute_reply.started":"2025-07-03T10:51:33.196937Z","shell.execute_reply":"2025-07-03T10:51:33.294628Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_10/3030471879.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['pricingInfo_isAccessTP'].fillna(0.0, inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Filled pricingInfo_isAccessTP with 0.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Frequent Flyer Status (object dtype) - Fill with NONE for users without FF status\ndf_filled['frequentFlyer'].fillna('NONE', inplace=True)\nprint(\"Filled frequentFlyer with NONE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:51:33.301364Z","iopub.execute_input":"2025-07-03T10:51:33.301586Z","iopub.status.idle":"2025-07-03T10:51:33.991330Z","shell.execute_reply.started":"2025-07-03T10:51:33.301564Z","shell.execute_reply":"2025-07-03T10:51:33.985430Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_10/1784150114.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['frequentFlyer'].fillna('NONE', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"Filled frequentFlyer with NONE\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# For airport codes (object dtype), we use mode imputation since forward fill\n# with object columns can be unreliable. SimpleImputer handles object dtypes well\n\nairport_cols_to_fill = [\n    'legs0_segments0_arrivalTo_airport_city_iata',\n    'legs0_segments0_arrivalTo_airport_iata', \n    'legs1_segments0_departureFrom_airport_iata'\n]\n\n# using simpleImputer for object columns - it handles NaN in object dtypes properly\nfor col in airport_cols_to_fill:\n    if df_filled[col].isnull().sum() > 0:\n        # Create imputer for this specific column\n        imputer = SimpleImputer(strategy='most_frequent')\n        # Reshape for sklearn (needs 2D array) and flatten back\n        df_filled[col] = imputer.fit_transform(df_filled[[col]]).ravel()\n\nprint(\"âœ“ Filled airport codes using mode imputation\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:51:33.993205Z","iopub.execute_input":"2025-07-03T10:51:33.993445Z","iopub.status.idle":"2025-07-03T10:51:49.591306Z","shell.execute_reply.started":"2025-07-03T10:51:33.993422Z","shell.execute_reply":"2025-07-03T10:51:49.584904Z"}},"outputs":[{"name":"stdout","text":"âœ“ Filled airport codes using mode imputation\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Return Flight Fields (One Way vs Round Trip) \n# These are datetime like objects, so we use string indicator instead of 0\nreturn_flight_object_cols = [\n    'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration'\n]\n\nfor col in return_flight_object_cols:\n    df_filled[col].fillna('NONE', inplace=True)\n\nprint(\"âœ“ Filled return flight timing fields with 'NONE'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:51:49.593221Z","iopub.execute_input":"2025-07-03T10:51:49.593474Z","iopub.status.idle":"2025-07-03T10:51:52.848836Z","shell.execute_reply.started":"2025-07-03T10:51:49.593449Z","shell.execute_reply":"2025-07-03T10:51:52.842915Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_10/886912719.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled[col].fillna('NONE', inplace=True)\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Filled return flight timing fields with 'NONE'\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Get all segment columns and separate by data type\nsegment_cols = [col for col in df_filled.columns if 'segments' in col]\n\n# Separate columns by dtype for appropriate filling\nobject_segment_cols = []\nfloat_segment_cols = []\n\nfor col in segment_cols:\n    if df_filled[col].dtype == 'object':\n        object_segment_cols.append(col)\n    elif df_filled[col].dtype == 'float64':\n        float_segment_cols.append(col)\n\n# Fill object segment fields with NONE (no connection exists)\nfor col in object_segment_cols:\n    df_filled.fillna({col: 'NONE'}, inplace=True)\n\n# Fill float segment fields with 0.0 (no connection/duration/seats/etc)\nfor col in float_segment_cols:\n    df_filled.fillna({col: '0.0'}, inplace=True)\n\nprint(f\"âœ“ Filled {len(object_segment_cols)} object segment columns with 'NONE'\")\nprint(f\"âœ“ Filled {len(float_segment_cols)} float segment columns with 0.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:51:52.851058Z","iopub.execute_input":"2025-07-03T10:51:52.851295Z","iopub.status.idle":"2025-07-03T10:56:04.594249Z","shell.execute_reply.started":"2025-07-03T10:51:52.851272Z","shell.execute_reply":"2025-07-03T10:56:04.589065Z"}},"outputs":[{"name":"stdout","text":"âœ“ Filled 64 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# All miniRules columns are float64, so we use appropriate float values\n\n# Monetary penalties (float64) - Fill with 0.0 (no monetary penalty)\ndf_filled['miniRules0_monetaryAmount'].fillna(0.0, inplace=True)  # Cancellation\ndf_filled['miniRules1_monetaryAmount'].fillna(0.0, inplace=True)  # Exchange\n\n# Percentage penalties (float64) - Fill with 100.0 (100% penalty = no refund/exchange)\n# This is more conservative and realistic for restrictive tickets\ndf_filled['miniRules0_percentage'].fillna(100.0, inplace=True)    # Cancellation\ndf_filled['miniRules1_percentage'].fillna(100.0, inplace=True)    # Exchange\n\n# Status information (float64) - Fill with 0.0 (not allowed)\ndf_filled['miniRules0_statusInfos'].fillna(0.0, inplace=True)     # Cancellation\ndf_filled['miniRules1_statusInfos'].fillna(0.0, inplace=True)     # Exchange\n\nprint(\"âœ“ Filled cancellation and exchange rules with appropriate float values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:56:04.595586Z","iopub.execute_input":"2025-07-03T10:56:04.595830Z","iopub.status.idle":"2025-07-03T10:56:05.348935Z","shell.execute_reply.started":"2025-07-03T10:56:04.595807Z","shell.execute_reply":"2025-07-03T10:56:05.343895Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_10/1464864012.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['miniRules0_monetaryAmount'].fillna(0.0, inplace=True)  # Cancellation\n/tmp/ipykernel_10/1464864012.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['miniRules1_monetaryAmount'].fillna(0.0, inplace=True)  # Exchange\n/tmp/ipykernel_10/1464864012.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['miniRules0_percentage'].fillna(100.0, inplace=True)    # Cancellation\n/tmp/ipykernel_10/1464864012.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['miniRules1_percentage'].fillna(100.0, inplace=True)    # Exchange\n/tmp/ipykernel_10/1464864012.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['miniRules0_statusInfos'].fillna(0.0, inplace=True)     # Cancellation\n/tmp/ipykernel_10/1464864012.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df_filled['miniRules1_statusInfos'].fillna(0.0, inplace=True)     # Exchange\n","output_type":"stream"},{"name":"stdout","text":"âœ“ Filled cancellation and exchange rules with appropriate float values\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Remaining missing values\nremaining_missing = df_filled.isnull().sum()\ntotal_remaining = remaining_missing.sum()\n\nprint(f\"\\n=== IMPUTATION SUMMARY ===\")\nprint(f\"Original missing values: {df.isnull().sum().sum():,}\")\nprint(f\"Remaining missing values: {total_remaining:,}\")\nprint(f\"Successfully filled: {df.isnull().sum().sum() - total_remaining:,} values\")\n\n# Show columns that still have missing values (if any)\nif total_remaining > 0:\n    print(f\"\\nColumns still with missing values:\")\n    print(remaining_missing[remaining_missing > 0])\nelse:\n    print(\"\\nðŸŽ‰ All missing values have been successfully filled!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:56:05.351222Z","iopub.execute_input":"2025-07-03T10:56:05.351457Z","iopub.status.idle":"2025-07-03T10:58:26.751455Z","shell.execute_reply.started":"2025-07-03T10:56:05.351434Z","shell.execute_reply":"2025-07-03T10:58:26.747307Z"}},"outputs":[{"name":"stdout","text":"\n=== IMPUTATION SUMMARY ===\nOriginal missing values: 1,352,535,929\nRemaining missing values: 0\nSuccessfully filled: 1,352,535,929 values\n\nðŸŽ‰ All missing values have been successfully filled!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Data quality checks \nprint(f\"\\n=== DATA QUALITY CHECKS ===\")\nprint(f\"DataFrame shape: {df_filled.shape}\")\n\n# Check data types after imputation\nprint(f\"\\nData types after imputation:\")\ndtype_changes = []\nfor col in df_filled.columns:\n    if df[col].dtype != df_filled[col].dtype:\n        dtype_changes.append(f\"{col}: {df[col].dtype} â†’ {df_filled[col].dtype}\")\n\nif dtype_changes:\n    print(\"Data type changes:\")\n    for change in dtype_changes[:5]:  # Show first 5 changes\n        print(f\"  {change}\")\n    if len(dtype_changes) > 5:\n        print(f\"  ... and {len(dtype_changes) - 5} more\")\nelse:\n    print(\"No data type changes occurred\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:58:26.755272Z","iopub.execute_input":"2025-07-03T10:58:26.755525Z","iopub.status.idle":"2025-07-03T10:58:26.777860Z","shell.execute_reply.started":"2025-07-03T10:58:26.755501Z","shell.execute_reply":"2025-07-03T10:58:26.771585Z"}},"outputs":[{"name":"stdout","text":"\n=== DATA QUALITY CHECKS ===\nDataFrame shape: (18145372, 127)\n\nData types after imputation:\nData type changes:\n  corporateTariffCode: Int64 â†’ int64\n  legs0_segments0_baggageAllowance_quantity: float64 â†’ object\n  legs0_segments0_baggageAllowance_weightMeasurementType: float64 â†’ object\n  legs0_segments0_seatsAvailable: float64 â†’ object\n  legs0_segments1_baggageAllowance_quantity: float64 â†’ object\n  ... and 27 more\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Check filled values\nprint(f\"\\nSample of filled values:\")\nprint(f\"corporateTariffCode range: {df_filled['corporateTariffCode'].min()} to {df_filled['corporateTariffCode'].max()}\")\nprint(f\"frequentFlyer unique values: {df_filled['frequentFlyer'].nunique()}\")\nprint(f\"miniRules0_percentage range: {df_filled['miniRules0_percentage'].min():.1f} to {df_filled['miniRules0_percentage'].max():.1f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:58:26.780283Z","iopub.execute_input":"2025-07-03T10:58:26.780539Z","iopub.status.idle":"2025-07-03T10:58:27.814401Z","shell.execute_reply.started":"2025-07-03T10:58:26.780514Z","shell.execute_reply":"2025-07-03T10:58:27.807599Z"}},"outputs":[{"name":"stdout","text":"\nSample of filled values:\ncorporateTariffCode range: -1 to 181\nfrequentFlyer unique values: 372\nminiRules0_percentage range: 0.0 to 100.0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Check for any unexpected data types\nprint(f\"\\nFinal data types summary:\")\nprint(df_filled.dtypes.value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:58:27.815290Z","iopub.execute_input":"2025-07-03T10:58:27.815536Z","iopub.status.idle":"2025-07-03T10:58:27.828247Z","shell.execute_reply.started":"2025-07-03T10:58:27.815502Z","shell.execute_reply":"2025-07-03T10:58:27.822418Z"}},"outputs":[{"name":"stdout","text":"\nFinal data types summary:\nobject            104\nfloat64            10\nint64               8\nbool                4\ndatetime64[ns]      1\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"print(f\"\\n=== IMPORTANT NOTES ===\")\nprint(\"Data type considerations:\")\nprint(\"â€¢ Int64 â†’ int64: Converted nullable integer to regular integer with -1 as null indicator\")\nprint(\"â€¢ object columns: Used 'NONE' as null indicator for categorical/string data\")  \nprint(\"â€¢ float64 columns: Used 0.0 for numeric nulls, 100.0 for percentage penalties\")\nprint(\"â€¢ datetime objects: Used 'NONE' string indicator (could convert to datetime later)\")\n\nprint(f\"\\nâœ… Dataset is ready for further analysis!\")\nprint(f\"Use 'df_filled' for your clean dataset with no missing values\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:58:27.830713Z","iopub.execute_input":"2025-07-03T10:58:27.830972Z","iopub.status.idle":"2025-07-03T10:58:27.846282Z","shell.execute_reply.started":"2025-07-03T10:58:27.830951Z","shell.execute_reply":"2025-07-03T10:58:27.839891Z"}},"outputs":[{"name":"stdout","text":"\n=== IMPORTANT NOTES ===\nData type considerations:\nâ€¢ Int64 â†’ int64: Converted nullable integer to regular integer with -1 as null indicator\nâ€¢ object columns: Used 'NONE' as null indicator for categorical/string data\nâ€¢ float64 columns: Used 0.0 for numeric nulls, 100.0 for percentage penalties\nâ€¢ datetime objects: Used 'NONE' string indicator (could convert to datetime later)\n\nâœ… Dataset is ready for further analysis!\nUse 'df_filled' for your clean dataset with no missing values\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"print(\"\\n1. Converting datetime columns...\")\ndatetime_columns = [\n    'legs0_arrivalAt', 'legs0_departureAt',\n    'legs1_arrivalAt', 'legs1_departureAt'\n]\n\n# Don't convert 'NONE' to NaT, keep as string or replace with valid datetime\nfor col in datetime_columns:\n    if col in df_filled.columns:\n        print(f\"Processing {col}...\")\n        # Option 1: Skip rows with 'NONE' entirely\n        mask = df_filled[col] != 'NONE'\n        df_filled.loc[mask, col] = pd.to_datetime(df_filled.loc[mask, col], errors='coerce')\n        # Leave 'NONE' as string (don't convert to datetime)\n\nprint(\"Datetime conversion completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:58:27.849000Z","iopub.execute_input":"2025-07-03T10:58:27.849267Z","iopub.status.idle":"2025-07-03T11:02:06.180046Z","shell.execute_reply.started":"2025-07-03T10:58:27.849243Z","shell.execute_reply":"2025-07-03T11:02:06.173763Z"}},"outputs":[{"name":"stdout","text":"\n1. Converting datetime columns...\nProcessing legs0_arrivalAt...\nProcessing legs0_departureAt...\nProcessing legs1_arrivalAt...\nProcessing legs1_departureAt...\nDatetime conversion completed\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"\\n3. Optimizing categorical variables...\")\n\n# Convert string categories to categorical dtype for memory efficiency\ncategorical_columns = [\n    'frequentFlyer', 'searchRoute', 'ranker_id'\n]\n\n# Add airport and airline code columns\nairport_airline_cols = [col for col in df_filled.columns \n                       if any(keyword in col for keyword in ['iata', 'code', 'flightNumber']) \n                       and df_filled[col].dtype == 'object']\ncategorical_columns.extend(airport_airline_cols)\n\nfor col in categorical_columns:\n    if col in df_filled.columns:\n        # Convert to categorical if it has reasonable number of unique values\n        unique_count = df_filled[col].nunique()\n        total_count = len(df_filled)\n        \n        # Convert to categorical if less than 50% unique values (arbitrary threshold)\n        if unique_count / total_count < 0.5:\n            df_filled[col] = df_filled[col].astype('category')\n            print(f\"   âœ“ {col}: {unique_count} categories\")\n        else:\n            print(f\"   â†’ {col}: Too many unique values ({unique_count}), keeping as object\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:02:06.182243Z","iopub.execute_input":"2025-07-03T11:02:06.182495Z","iopub.status.idle":"2025-07-03T11:04:20.147492Z","shell.execute_reply.started":"2025-07-03T11:02:06.182472Z","shell.execute_reply":"2025-07-03T11:04:20.141776Z"}},"outputs":[{"name":"stdout","text":"\n3. Optimizing categorical variables...\n   âœ“ frequentFlyer: 372 categories\n   âœ“ searchRoute: 5769 categories\n   âœ“ ranker_id: 105539 categories\n   âœ“ legs0_segments0_aircraft_code: 108 categories\n   âœ“ legs0_segments0_arrivalTo_airport_city_iata: 500 categories\n   âœ“ legs0_segments0_arrivalTo_airport_iata: 534 categories\n   âœ“ legs0_segments0_departureFrom_airport_iata: 394 categories\n   âœ“ legs0_segments0_flightNumber: 7531 categories\n   âœ“ legs0_segments0_marketingCarrier_code: 164 categories\n   âœ“ legs0_segments0_operatingCarrier_code: 220 categories\n   âœ“ legs0_segments1_aircraft_code: 103 categories\n   âœ“ legs0_segments1_arrivalTo_airport_city_iata: 425 categories\n   âœ“ legs0_segments1_arrivalTo_airport_iata: 476 categories\n   âœ“ legs0_segments1_departureFrom_airport_iata: 455 categories\n   âœ“ legs0_segments1_flightNumber: 6577 categories\n   âœ“ legs0_segments1_marketingCarrier_code: 153 categories\n   âœ“ legs0_segments1_operatingCarrier_code: 207 categories\n   âœ“ legs0_segments2_aircraft_code: 82 categories\n   âœ“ legs0_segments2_arrivalTo_airport_city_iata: 243 categories\n   âœ“ legs0_segments2_arrivalTo_airport_iata: 287 categories\n   âœ“ legs0_segments2_departureFrom_airport_iata: 258 categories\n   âœ“ legs0_segments2_flightNumber: 3407 categories\n   âœ“ legs0_segments2_marketingCarrier_code: 95 categories\n   âœ“ legs0_segments2_operatingCarrier_code: 132 categories\n   âœ“ legs0_segments3_aircraft_code: 2 categories\n   âœ“ legs0_segments3_arrivalTo_airport_city_iata: 3 categories\n   âœ“ legs0_segments3_arrivalTo_airport_iata: 3 categories\n   âœ“ legs0_segments3_departureFrom_airport_iata: 6 categories\n   âœ“ legs0_segments3_flightNumber: 8 categories\n   âœ“ legs0_segments3_marketingCarrier_code: 5 categories\n   âœ“ legs0_segments3_operatingCarrier_code: 5 categories\n   âœ“ legs1_segments0_aircraft_code: 94 categories\n   âœ“ legs1_segments0_arrivalTo_airport_city_iata: 313 categories\n   âœ“ legs1_segments0_arrivalTo_airport_iata: 340 categories\n   âœ“ legs1_segments0_departureFrom_airport_iata: 344 categories\n   âœ“ legs1_segments0_flightNumber: 5377 categories\n   âœ“ legs1_segments0_marketingCarrier_code: 145 categories\n   âœ“ legs1_segments0_operatingCarrier_code: 187 categories\n   âœ“ legs1_segments1_aircraft_code: 85 categories\n   âœ“ legs1_segments1_arrivalTo_airport_city_iata: 181 categories\n   âœ“ legs1_segments1_arrivalTo_airport_iata: 195 categories\n   âœ“ legs1_segments1_departureFrom_airport_iata: 299 categories\n   âœ“ legs1_segments1_flightNumber: 3222 categories\n   âœ“ legs1_segments1_marketingCarrier_code: 119 categories\n   âœ“ legs1_segments1_operatingCarrier_code: 137 categories\n   âœ“ legs1_segments2_aircraft_code: 43 categories\n   âœ“ legs1_segments2_arrivalTo_airport_city_iata: 61 categories\n   âœ“ legs1_segments2_arrivalTo_airport_iata: 68 categories\n   âœ“ legs1_segments2_departureFrom_airport_iata: 86 categories\n   âœ“ legs1_segments2_flightNumber: 381 categories\n   âœ“ legs1_segments2_marketingCarrier_code: 55 categories\n   âœ“ legs1_segments2_operatingCarrier_code: 62 categories\n   âœ“ legs1_segments3_aircraft_code: 2 categories\n   âœ“ legs1_segments3_arrivalTo_airport_city_iata: 2 categories\n   âœ“ legs1_segments3_arrivalTo_airport_iata: 2 categories\n   âœ“ legs1_segments3_departureFrom_airport_iata: 2 categories\n   âœ“ legs1_segments3_flightNumber: 2 categories\n   âœ“ legs1_segments3_marketingCarrier_code: 2 categories\n   âœ“ legs1_segments3_operatingCarrier_code: 2 categories\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"print(\"\\n6. Performing data validation...\")\n\n# Check for any remaining missing values\nmissing_check = df_filled.isnull().sum()\nif missing_check.sum() > 0:\n    print(f\"   âš ï¸  Warning: {missing_check.sum()} missing values remain:\")\n    print(missing_check[missing_check > 0].head())\nelse:\n    print(\"   âœ“ No missing values found\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:04:20.148460Z","iopub.execute_input":"2025-07-03T11:04:20.148731Z","iopub.status.idle":"2025-07-03T11:04:55.052215Z","shell.execute_reply.started":"2025-07-03T11:04:20.148703Z","shell.execute_reply":"2025-07-03T11:04:55.048047Z"}},"outputs":[{"name":"stdout","text":"\n6. Performing data validation...\n   âœ“ No missing values found\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Check for infinite values\ninf_cols = []\nfor col in df_filled.select_dtypes(include=[np.number]).columns:\n    if np.isinf(df_filled[col]).any():\n        inf_cols.append(col)\n\nif inf_cols:\n    print(f\"   âš ï¸  Warning: Infinite values found in: {inf_cols}\")\n    # Replace inf with large finite values\n    for col in inf_cols:\n        df_filled[col] = df_filled[col].replace([np.inf, -np.inf], [df_filled[col].max()*10, df_filled[col].min()*10])\n    print(\"   âœ“ Infinite values replaced\")\nelse:\n    print(\"   âœ“ No infinite values found\")\n\n# Check data ranges\nprint(\"   Data range validation:\")\nprint(f\"   â€¢ Total price range: ${df_filled['totalPrice'].min():.2f} - ${df_filled['totalPrice'].max():.2f}\")\nprint(f\"   â€¢ Flight duration range: {df_filled['legs0_duration'].min():} - {df_filled['legs0_duration'].max():} minutes\")\nprint(f\"   â€¢ Passenger count range: {df_filled['pricingInfo_passengerCount'].min()} - {df_filled['pricingInfo_passengerCount'].max()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:04:55.056057Z","iopub.execute_input":"2025-07-03T11:04:55.057963Z","iopub.status.idle":"2025-07-03T11:05:00.490125Z","shell.execute_reply.started":"2025-07-03T11:04:55.057930Z","shell.execute_reply":"2025-07-03T11:05:00.484093Z"}},"outputs":[{"name":"stdout","text":"   âœ“ No infinite values found\n   Data range validation:\n   â€¢ Total price range: $770.00 - $9944355.00\n   â€¢ Flight duration range: 00:30:00 - 6.03:15:00 minutes\n   â€¢ Passenger count range: 1 - 1\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(\"\\n7. Optimizing memory usage...\")\n\n# Calculate memory usage before and after\nmemory_before = df_filled.memory_usage(deep=True).sum() / (1024**2)  # MB\n\n# Additional memory optimizations for float columns\nfloat_cols = df_filled.select_dtypes(include=['float64']).columns\nfor col in float_cols:\n    if col not in ['totalPrice', 'taxes']:  # Keep price columns as float32\n        # Check if we can downcast to float32\n        col_min, col_max = df_filled[col].min(), df_filled[col].max()\n        if col_min >= np.finfo(np.float32).min and col_max <= np.finfo(np.float32).max:\n            df_filled[col] = df_filled[col].astype('float32')\n\nmemory_after = df_filled.memory_usage(deep=True).sum() / (1024**2)  # MB\nmemory_saved = memory_before - memory_after\n\nprint(f\"   Memory usage: {memory_before:.1f} MB â†’ {memory_after:.1f} MB\")\nprint(f\"   Memory saved: {memory_saved:.1f} MB ({memory_saved/memory_before*100:.1f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# For leg durations\ndf_filled = df_filled.rename(columns={\n    'legs0_duration_minutes': 'leg0_total_minutes',\n    'legs1_duration_minutes': 'leg1_total_minutes'\n})\n\n# For segment durations - remove the redundant suffix\nfor col in df_filled.columns:\n    if '_minutes_minutes' in col:\n        new_name = col.replace('_minutes_minutes', '_minutes')\n        df_filled = df_filled.rename(columns={col: new_name})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:05:00.492086Z","iopub.execute_input":"2025-07-03T11:05:00.492359Z","iopub.status.idle":"2025-07-03T11:06:21.240013Z","shell.execute_reply.started":"2025-07-03T11:05:00.492333Z","shell.execute_reply":"2025-07-03T11:06:21.234515Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Show all columns in current session\npd.set_option('display.max_columns', None)\n\n# Now when you view your DataFrame, all columns will show\nprint(df.columns.tolist())  # List of all columns\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group Analysis (ranker_id)\ngroup_sizes = df_filled.groupby(\"ranker_id\").size().reset_index(name=\"group_size\")\nprint(group_sizes[\"group_size\"].describe().to_string())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use pandas filtering\ngroups_over_10 = group_sizes[group_sizes[\"group_size\"] > 10]\npercentage_over_10 = (len(groups_over_10) / len(group_sizes)) * 100\nprint(f\"\\nPercentage of queries (ranker_id) with >10 options: {percentage_over_10:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check results\nprint(f\"Original columns: {len(df_filled.columns)}\")\nprint(f\"Engineered columns: {len(df_engineered.columns)}\")\nprint(f\"NaN values: {df_engineered.isna().sum().sum()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# Data Preparation for Ranking Model (LightGBM + Optuna)\n# ===========================\n# This block handles data preprocessing and feature engineering for training a LightGBM ranking model.\n# The process includes:\n#\n# ðŸ”¹ Group-aware train/validation split using GroupShuffleSplit (grouped by `ranker_id`).\n# ðŸ”¹ Separation of target variable (`selected`) from features.\n# ðŸ”¹ Deduplication of feature columns to avoid model training issues.\n# ðŸ”¹ Extraction and transformation of datetime features with:\n#     - Hour, minute, weekday, and weekend flags.\n#     - Cyclical encoding (sin/cos) for hourly patterns â€” helpful for capturing temporal behavior.\n#\n# ðŸ‘‰ The model will later use group info (`ranker_id`) for learning to rank\n\nimport optuna\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupShuffleSplit\n# from sklearn.preprocessing import LabelEncoder\nimport time\n\n# Enhanced datetime feature extraction\ndef extract_datetime_features(df, time_cols):\n    for col in time_cols:\n        df[col + '_hour'] = df[col].dt.hour\n        df[col + '_minute'] = df[col].dt.minute\n        df[col + '_dayofweek'] = df[col].dt.dayofweek\n        df[col + '_is_weekend'] = (df[col].dt.dayofweek >= 5).astype(int)\n        # Add cyclic encoding for time features\n        df[col + '_hour_sin'] = np.sin(2 * np.pi * df[col + '_hour']/24)\n        df[col + '_hour_cos'] = np.cos(2 * np.pi * df[col + '_hour']/24)\n    return df.drop(columns=time_cols)\n\n# ----------------------------------\n# Metric: hitrate_at_3\n# ----------------------------------\n# Custom evaluation metric for ranking quality.\n# It checks if the correct flight was within the top-3 predictions in each session group.\n# \n# This aligns with typical top-K recommendation evaluation strategies and helps measure practical impact.\n\ndef hitrate_at_3(y_true, y_pred, group_sizes, k=3):\n    group_ids = np.repeat(np.arange(len(group_sizes)), group_sizes)\n\n    df = pd.DataFrame({\n        'group': group_ids,\n        'true': y_true,\n        'pred': y_pred\n    })\n\n    hitrate = (\n        df.groupby('group')\n        .apply(lambda g: g.nlargest(min(k, len(g)), 'pred')['true'].max())\n        .mean()\n    )\n\n    return hitrate if not np.isnan(hitrate) else 0.0\n\nprint(\"Preparing data...\")\nsession_groups = df_filled['ranker_id'].values\nX = df_filled.drop(columns=['selected', 'ranker_id'])\ny = df_filled['selected'].values\n\ngss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_idx, val_idx = next(gss.split(X, y, groups=session_groups))\n\nX_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()\ny_train, y_val = y[train_idx], y[val_idx]\n\ntrain_groups = df_filled.iloc[train_idx].groupby('ranker_id').size().values\nval_groups = df_filled.iloc[val_idx].groupby('ranker_id').size().values\n\n# Get ranker_id for group calculations\ntrain_ranker_ids = df_filled.iloc[train_idx]['ranker_id'].values\nval_ranker_ids = df_filled.iloc[val_idx]['ranker_id'].values\n\n# Drop duplicate columns\nX_train = X_train.loc[:, ~X_train.columns.duplicated()]\nX_val = X_val.loc[:, ~X_val.columns.duplicated()]\n\n# Handle datetime features\ndatetime_cols = X_train.select_dtypes(include=['datetime64']).columns.tolist()\nif datetime_cols:\n    print(f\"Processing datetime columns: {datetime_cols}\")\n    X_train = extract_datetime_features(X_train, datetime_cols)\n    X_val = extract_datetime_features(X_val, datetime_cols)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:06:21.242409Z","iopub.execute_input":"2025-07-03T11:06:21.242670Z","iopub.status.idle":"2025-07-03T11:08:51.816555Z","shell.execute_reply.started":"2025-07-03T11:06:21.242647Z","shell.execute_reply":"2025-07-03T11:08:51.810902Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"},{"name":"stdout","text":"Preparing data...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_10/299905486.py:49: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  train_groups = df_filled.iloc[train_idx].groupby('ranker_id').size().values\n/tmp/ipykernel_10/299905486.py:50: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  val_groups = df_filled.iloc[val_idx].groupby('ranker_id').size().values\n","output_type":"stream"},{"name":"stdout","text":"Processing datetime columns: ['requestDate']\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# Categorical Feature Encoding (Target Encoding)\n# ===========================\n# This step encodes categorical variables using **target encoding**, which replaces each category\n# with the mean target value for that category (with smoothing to avoid overfitting).\n#\n# ðŸ”¹ We use `category_encoders.TargetEncoder` with smoothing=1.0 to prevent data leakage and high variance on rare categories.\n# ðŸ”¹ The encoder is fitted **only on training data** to ensure validation remains untouched.\n# ðŸ”¹ This method is especially helpful when dealing with high-cardinality categories \n#\n# ðŸ‘‰ Always verify that target encoding doesnâ€™t introduce leakage by accidentally using label information from the validation/test set.\n\nimport category_encoders as ce\n\n# Handle categorical features with target encoding\nprint(\"Processing categorical features...\")\ncat_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\nif cat_cols:\n    print(f\"Categorical columns: {cat_cols}\")\n    te = ce.TargetEncoder(cols=cat_cols, smoothing=1.0)\n    X_train[cat_cols] = te.fit_transform(X_train[cat_cols], y_train)\n    X_val[cat_cols] = te.transform(X_val[cat_cols])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:08:51.818766Z","iopub.execute_input":"2025-07-03T11:08:51.819168Z","iopub.status.idle":"2025-07-03T11:28:54.753191Z","shell.execute_reply.started":"2025-07-03T11:08:51.819142Z","shell.execute_reply":"2025-07-03T11:28:54.746978Z"}},"outputs":[{"name":"stdout","text":"Processing categorical features...\nCategorical columns: ['frequentFlyer', 'legs0_arrivalAt', 'legs0_departureAt', 'legs0_duration', 'legs0_segments0_aircraft_code', 'legs0_segments0_arrivalTo_airport_city_iata', 'legs0_segments0_arrivalTo_airport_iata', 'legs0_segments0_baggageAllowance_quantity', 'legs0_segments0_baggageAllowance_weightMeasurementType', 'legs0_segments0_departureFrom_airport_iata', 'legs0_segments0_duration', 'legs0_segments0_flightNumber', 'legs0_segments0_marketingCarrier_code', 'legs0_segments0_operatingCarrier_code', 'legs0_segments0_seatsAvailable', 'legs0_segments1_aircraft_code', 'legs0_segments1_arrivalTo_airport_city_iata', 'legs0_segments1_arrivalTo_airport_iata', 'legs0_segments1_baggageAllowance_quantity', 'legs0_segments1_baggageAllowance_weightMeasurementType', 'legs0_segments1_cabinClass', 'legs0_segments1_departureFrom_airport_iata', 'legs0_segments1_duration', 'legs0_segments1_flightNumber', 'legs0_segments1_marketingCarrier_code', 'legs0_segments1_operatingCarrier_code', 'legs0_segments1_seatsAvailable', 'legs0_segments2_aircraft_code', 'legs0_segments2_arrivalTo_airport_city_iata', 'legs0_segments2_arrivalTo_airport_iata', 'legs0_segments2_baggageAllowance_quantity', 'legs0_segments2_baggageAllowance_weightMeasurementType', 'legs0_segments2_cabinClass', 'legs0_segments2_departureFrom_airport_iata', 'legs0_segments2_duration', 'legs0_segments2_flightNumber', 'legs0_segments2_marketingCarrier_code', 'legs0_segments2_operatingCarrier_code', 'legs0_segments2_seatsAvailable', 'legs0_segments3_aircraft_code', 'legs0_segments3_arrivalTo_airport_city_iata', 'legs0_segments3_arrivalTo_airport_iata', 'legs0_segments3_baggageAllowance_quantity', 'legs0_segments3_baggageAllowance_weightMeasurementType', 'legs0_segments3_cabinClass', 'legs0_segments3_departureFrom_airport_iata', 'legs0_segments3_duration', 'legs0_segments3_flightNumber', 'legs0_segments3_marketingCarrier_code', 'legs0_segments3_operatingCarrier_code', 'legs0_segments3_seatsAvailable', 'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration', 'legs1_segments0_aircraft_code', 'legs1_segments0_arrivalTo_airport_city_iata', 'legs1_segments0_arrivalTo_airport_iata', 'legs1_segments0_baggageAllowance_quantity', 'legs1_segments0_baggageAllowance_weightMeasurementType', 'legs1_segments0_cabinClass', 'legs1_segments0_departureFrom_airport_iata', 'legs1_segments0_duration', 'legs1_segments0_flightNumber', 'legs1_segments0_marketingCarrier_code', 'legs1_segments0_operatingCarrier_code', 'legs1_segments0_seatsAvailable', 'legs1_segments1_aircraft_code', 'legs1_segments1_arrivalTo_airport_city_iata', 'legs1_segments1_arrivalTo_airport_iata', 'legs1_segments1_baggageAllowance_quantity', 'legs1_segments1_baggageAllowance_weightMeasurementType', 'legs1_segments1_cabinClass', 'legs1_segments1_departureFrom_airport_iata', 'legs1_segments1_duration', 'legs1_segments1_flightNumber', 'legs1_segments1_marketingCarrier_code', 'legs1_segments1_operatingCarrier_code', 'legs1_segments1_seatsAvailable', 'legs1_segments2_aircraft_code', 'legs1_segments2_arrivalTo_airport_city_iata', 'legs1_segments2_arrivalTo_airport_iata', 'legs1_segments2_baggageAllowance_quantity', 'legs1_segments2_baggageAllowance_weightMeasurementType', 'legs1_segments2_cabinClass', 'legs1_segments2_departureFrom_airport_iata', 'legs1_segments2_duration', 'legs1_segments2_flightNumber', 'legs1_segments2_marketingCarrier_code', 'legs1_segments2_operatingCarrier_code', 'legs1_segments2_seatsAvailable', 'legs1_segments3_aircraft_code', 'legs1_segments3_arrivalTo_airport_city_iata', 'legs1_segments3_arrivalTo_airport_iata', 'legs1_segments3_baggageAllowance_quantity', 'legs1_segments3_baggageAllowance_weightMeasurementType', 'legs1_segments3_cabinClass', 'legs1_segments3_departureFrom_airport_iata', 'legs1_segments3_duration', 'legs1_segments3_flightNumber', 'legs1_segments3_marketingCarrier_code', 'legs1_segments3_operatingCarrier_code', 'legs1_segments3_seatsAvailable', 'searchRoute']\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# Hyperparameter Tuning: LGBMRanker with Optuna\n# ===========================\n# We use Optuna to optimize a LightGBM LambdaRank model for ranking flight options.\n# The objective is to **maximize HitRate@3**, a custom evaluation metric aligned with the business goal\n# (whether the correct option appears in the top 3 predicted).\n#\n# ðŸ”¹ `objective`: 'lambdarank' optimizes pairwise ranking with NDCG.\n# ðŸ”¹ Optuna searches over:\n#     - learning_rate, num_leaves, regularization (L1/L2), and data subsampling params\n# ðŸ”¹ Uses Group data (`ranker_id`) via LightGBM's `group` and `eval_group` arguments.\n# ðŸ”¹ `n_estimators` is set high (2000) to allow early stopping to find optimal iteration count.\n#\n# âš ï¸ Important:\n#     - We ensure reproducibility with a fixed seed (TPESampler).\n#     - Unpromising trials are pruned using `MedianPruner` to speed up tuning.\n#     - Categorical columns are passed explicitly (if available).\n#\n# ðŸ‘‰ Final best parameters and score are printed after optimization completes.\n\n# ----------------------------------\n# Running the Optuna Study\n# ----------------------------------\n# - direction='maximize': because higher HitRate@3 is better\n# - n_trials=50: can increase for better search (if time/memory allows)\n# - timeout=24h: ensures overnight runs terminate\n# - show_progress_bar=True: helpful in notebooks/CLI\n\nfrom lightgbm import LGBMRanker\n\ndef objective(trial):\n    params = {\n        'objective': 'lambdarank',\n        'metric': 'ndcg',\n        'ndcg_eval_at': [3],\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 16, 128, step=8),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 200, step=10),\n        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0, step=0.1),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0, step=0.1),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n        'force_row_wise': True,\n        'random_state': 42,\n        'verbosity': -1,\n        'n_estimators': 2000,  # Increased to ensure early stopping decides actual trees\n    }\n\n    model = LGBMRanker(**params)\n\n    model.fit(\n        X_train, y_train,\n        group=train_groups,\n        eval_set=[(X_val, y_val)],\n        eval_group=[val_groups],\n        categorical_feature=cat_cols\n    )\n\n    y_val_pred = model.predict(X_val)\n    hitrate = hitrate_at_3(y_val, y_val_pred, val_groups)\n    return hitrate\n\n# Configure and run Optuna study\nstudy = optuna.create_study(\n    direction='maximize',\n    sampler=optuna.samplers.TPESampler(seed=42),  # For reproducible optimization\n    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)  # Prune unpromising trials\n)\nstudy.optimize(objective, n_trials=50, timeout=24*3600, show_progress_bar=True)\n\n# Output results\nprint(\"\\n\" + \"=\"*50)\nprint(f\"âœ… Best HitRate@3: {study.best_value:.4f}\")\nprint(\"âœ… Best Params:\", study.best_params)\nprint(\"=\"*50)\nfor key, value in study.best_params.items():\n    print(f\"  - {key}: {value}\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T13:28:59.846867Z","iopub.execute_input":"2025-07-03T13:28:59.847144Z","iopub.status.idle":"2025-07-03T13:28:59.861950Z","shell.execute_reply.started":"2025-07-03T13:28:59.847119Z","shell.execute_reply":"2025-07-03T13:28:59.856942Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ===========================\n# Final Model Training with Best Parameters (LGBMRanker)\n# ===========================\n# Trains the final LightGBM LambdaRank model using the best hyperparameters obtained from Optuna.\n#\n# ðŸ”¹ `best_params` holds the tuned config, manually selected or copied from the best trial.\n# ðŸ”¹ We use LightGBM's built-in early stopping to prevent overfitting, monitoring NDCG@3 on validation data.\n# ðŸ”¹ Group information (`ranker_id`) is passed explicitly via `group` and `eval_group`.\n#\n# âœ… Model is saved using joblib for portability and reusability.\n\n# 2. Set best parameters from Optuna\nbest_params = {\n    'objective': 'lambdarank',\n    'metric': 'ndcg',\n    'eval_at': [3],\n    'boosting_type': 'gbdt',\n    'n_estimators': 1000,\n    'learning_rate': 0.05,\n    'num_leaves': 63,\n    'max_depth': -1,  # No limit\n    'min_child_samples': 20,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'max_bin': 255,\n    'random_state': 42,\n    'n_jobs': -1,\n    'importance_type': 'gain',\n    'verbose': -1,\n    'seed': 42\n}\n\n# train_set = lgb.Dataset(X_train, label=y_train, group=train_groups, categorical_feature=cat_cols)\n# val_set = lgb.Dataset(X_val, label=y_val, group=val_groups, reference=train_set, categorical_feature=cat_cols)\n\nfrom lightgbm import LGBMRanker\n\n# 1. Define model\nranker = LGBMRanker(\n    **best_params,\n)\n\n# 2. Fit the model\nranker.fit(\n    X_train,\n    y_train,\n    group=train_groups,\n    eval_set=[(X_val, y_val)],\n    eval_group=[val_groups],\n    # eval_at=[3],\n    # categorical_feature=cat_cols,\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=50, verbose=True),\n        lgb.log_evaluation(period=100)\n    ]\n)\nimport joblib\n\n# Save the model to a file\njoblib.dump(ranker, \"lgb_ranker_model3.pkl\")\nprint(\"âœ… Model saved to lgb_ranker_model.pkl\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:28:54.755049Z","iopub.execute_input":"2025-07-03T11:28:54.755411Z","iopub.status.idle":"2025-07-03T11:37:12.878131Z","shell.execute_reply.started":"2025-07-03T11:28:54.755388Z","shell.execute_reply":"2025-07-03T11:37:12.874644Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n","output_type":"stream"},{"name":"stdout","text":"Training until validation scores don't improve for 50 rounds\n[100]\tvalid_0's ndcg@3: 0.961296\n[200]\tvalid_0's ndcg@3: 0.962492\n[300]\tvalid_0's ndcg@3: 0.962976\n[400]\tvalid_0's ndcg@3: 0.963245\n[500]\tvalid_0's ndcg@3: 0.963473\n[600]\tvalid_0's ndcg@3: 0.963649\n[700]\tvalid_0's ndcg@3: 0.963787\n[800]\tvalid_0's ndcg@3: 0.963898\n[900]\tvalid_0's ndcg@3: 0.964114\n[1000]\tvalid_0's ndcg@3: 0.964242\nDid not meet early stopping. Best iteration is:\n[989]\tvalid_0's ndcg@3: 0.964243\nâœ… Model saved to lgb_ranker_model.pkl\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # # Use your existing hitrate_at_3 function\n# # def hitrate_at_3(y_true, y_pred):\n# #     \"\"\"\n# #     Calculate hitrate@3 metric.\n# #     Args:\n# #         y_true: List of sets/lists containing ground truth relevant items for each user.\n# #         y_pred: List of lists of recommended items (ranked in order) for each user\n# #     Returns:\n# #         hitrate@3 score (float) in the range [0, 1].\n# #     \"\"\"\n# #     total_hits = 0\n# #     num_users = len(y_true)\n# #     for true_items, preds in zip(y_true, y_pred):\n# #         true_set = set(true_items)\n# #         if any(item in true_set for item in preds[:3]):\n# #             total_hits += 1\n# #     return total_hits / num_users\n\n# def calculate_hit_rate_at_3(df_preds_with_true_and_rank):\n#     \"\"\"\n#     Calculates HitRate@3.\n#     df_preds_with_true_and_rank must have:\n#         - 'ranker_id'\n#         - 'selected' (true binary target, 1 for chosen)\n#         - 'predicted_rank' (rank assigned by the model, 1 is best)\n#     \"\"\"\n#     hits = 0\n#     valid_queries_count = 0\n    \n#     for ranker_id, group in df_preds_with_true_and_rank.groupby('ranker_id'):\n#         if len(group) <= 10:\n#             continue  # Skip groups with 10 or fewer options as per competition rules\n        \n#         valid_queries_count += 1\n        \n#         true_selected_item = group[group['selected'] == 1]\n        \n#         if not true_selected_item.empty:\n#             # Get the rank of the true selected item\n#             rank_of_true_item = true_selected_item.iloc[0]['predicted_rank']\n#             if rank_of_true_item <= 3:\n#                 hits += 1\n#         # else:\n#             # This shouldn't happen in validation if data is prepared correctly from train\n#             # print(f\"Warning: No selected item found for ranker_id {ranker_id} in HitRate calculation.\")\n            \n#     if valid_queries_count == 0:\n#         return 0.0\n#     return hits / valid_queries_count","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ===========================\n# Preprocessing Test Data (Same as Training Pipeline)\n# ===========================\n# Applies the same preprocessing steps used for training:\n# ðŸ”¹ Datetime feature extraction\n# ðŸ”¹ Target/categorical encoding using pre-fitted encoder\n# ðŸ”¹ Column alignment and cleanup\n#\n# ðŸ‘‰ Ensures consistency between training and inference data.\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load test data\ntest_path = \"/kaggle/input/aeroclub-recsys-cup-2025-1/test.parquet\"\n\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nimport duckdb\nimport pandas as pd\n\ntrain_path = \"/kaggle/input/aeroclub-recsys-cup-2025-1/train.parquet\"\ntest_path = \"/kaggle/input/aeroclub-recsys-cup-2025-1/test.parquet\"\n\ncon = duckdb.connect()\n\n# # Correct session sampling with minimum flight options\n# query = f\"\"\"\n#     WITH valid_sessions AS (\n#         SELECT ranker_id\n#         FROM parquet_scan('{test_path}')\n#         GROUP BY ranker_id\n#         HAVING COUNT(*) >= 3  -- Sessions with at least 10 flight options\n#     ),\n#     sampled_sessions AS (\n#         SELECT ranker_id\n#         FROM valid_sessions\n#         USING SAMPLE 55000  -- Sample 5000 sessions (adjust based on memory)\n#     )\n#     SELECT t.*\n#     FROM parquet_scan('{test_path}') t\n#     JOIN sampled_sessions s ON t.ranker_id = s.ranker_id\n# \"\"\"\n\nquery = f\"\"\"\n    SELECT *\n    FROM parquet_scan('{test_path}')\n\"\"\"\n\n# Execute query and convert to DataFrame\ndf = con.query(query).to_df()\ncon.close()\n\ndf_test = df\n\n# Check session stats\nsession_counts = df_test['ranker_id'].value_counts()\nprint(f\"Loaded {len(df_test)} rows from {len(session_counts)} sessions\")\nprint(f\"Min options per session: {session_counts.min()}\")\nprint(f\"Max options per session: {session_counts.max()}\")\n\nprint(\"Starting missing value imputation...\")\nprint(f\"Original missing values: {df_test.isnull().sum().sum()}\")\n\ndf_test['corporateTariffCode'] = df_test['corporateTariffCode'].fillna(-1).astype('int64')\nprint(\"âœ“ Filled corporateTariffCode with -1 (converted to int64)\")\n\ndf_test['pricingInfo_isAccessTP'].fillna(0.0, inplace=True)\nprint(\"âœ“ Filled pricingInfo_isAccessTP with 0.0\")\n\ndf_test['frequentFlyer'].fillna('NONE', inplace=True)\nprint(\"Filled frequentFlyer with NONE\")\n\nairport_cols_to_fill = [\n'legs0_segments0_arrivalTo_airport_city_iata',\n'legs0_segments0_arrivalTo_airport_iata',\n'legs1_segments0_departureFrom_airport_iata'\n]\n\nreturn_flight_object_cols = [\n'legs1_arrivalAt', 'legs1_departureAt', 'legs1_duration'\n]\n\nfor col in return_flight_object_cols:\n    df_test[col].fillna('NONE', inplace=True)\n\n    print(\"âœ“ Filled return flight timing fields with 'NONE'\")\n\n# Get all segment columns and separate by data type\n\nsegment_cols = [col for col in df_test.columns if 'segments' in col]\n\n# Separate columns by dtype for appropriate filling\n\nobject_segment_cols = []\nfloat_segment_cols = []\n\nfor col in segment_cols:\n    if df_test[col].dtype == 'object':\n        object_segment_cols.append(col)\n    elif df_test[col].dtype == 'float64':\n        float_segment_cols.append(col)\n\n# Fill object segment fields with NONE (no connection exists)\n\nfor col in object_segment_cols:\n    df_test.fillna({col: 'NONE'}, inplace=True)\n\n# Fill float segment fields with 0.0 (no connection/duration/seats/etc)\n\nfor col in float_segment_cols:\n    df_test.fillna({col: '0.0'}, inplace=True)\n\n    print(f\"âœ“ Filled {len(object_segment_cols)} object segment columns with 'NONE'\")\n    print(f\"âœ“ Filled {len(float_segment_cols)} float segment columns with 0.0\")\n\n# All miniRules columns are float64, so we use appropriate float values\n\n# Monetary penalties (float64) - Fill with 0.0 (no monetary penalty)\n\ndf_test['miniRules0_monetaryAmount'].fillna(0.0, inplace=True)  # Cancellation\ndf_test['miniRules1_monetaryAmount'].fillna(0.0, inplace=True)  # Exchange\n\n# Percentage penalties (float64) - Fill with 100.0 (100% penalty = no refund/exchange)\n\n# This is more conservative and realistic for restrictive tickets\n\ndf_test['miniRules0_percentage'].fillna(100.0, inplace=True)    # Cancellation\ndf_test['miniRules1_percentage'].fillna(100.0, inplace=True)    # Exchange\n\n# Status information (float64) - Fill with 0.0 (not allowed)\n\ndf_test['miniRules0_statusInfos'].fillna(0.0, inplace=True)     # Cancellation\ndf_test['miniRules1_statusInfos'].fillna(0.0, inplace=True)     # Exchange\n\nprint(\"âœ“ Filled cancellation and exchange rules with appropriate float values\")\n\nprint(\"\\n1. Converting datetime columns...\")\ndatetime_columns = [\n'legs0_arrivalAt', 'legs0_departureAt',\n'legs1_arrivalAt', 'legs1_departureAt'\n]\n\n# Don't convert 'NONE' to NaT, keep as string or replace with valid datetime\n\nfor col in datetime_columns:\n    if col in df_test.columns:\n        print(f\"Processing {col}...\")\n        # Option 1: Skip rows with 'NONE' entirely\n        mask = df_test[col] != 'NONE'\n        df_test.loc[mask, col] = pd.to_datetime(df_test.loc[mask, col], errors='coerce')\n        # Leave 'NONE' as string (don't convert to datetime)\n\nprint(\"Datetime conversion completed\")\n\nprint(\"\\n3. Optimizing categorical variables...\")\n\n# Convert string categories to categorical dtype for memory efficiency\ncategorical_columns = [\n    'frequentFlyer', 'searchRoute', 'ranker_id'\n]\n\n# Add airport and airline code columns\nairport_airline_cols = [col for col in df_test.columns \n                       if any(keyword in col for keyword in ['iata', 'code', 'flightNumber']) \n                       and df_test[col].dtype == 'object']\ncategorical_columns.extend(airport_airline_cols)\n\nfor col in categorical_columns:\n    if col in df_test.columns:\n        # Convert to categorical if it has reasonable number of unique values\n        unique_count = df_test[col].nunique()\n        total_count = len(df_test)\n        \n        # Convert to categorical if less than 50% unique values (arbitrary threshold)\n        if unique_count / total_count < 0.5:\n            df_test[col] = df_test[col].astype('category')\n            print(f\"   âœ“ {col}: {unique_count} categories\")\n        else:\n            print(f\"   â†’ {col}: Too many unique values ({unique_count}), keeping as object\")\n\n# If you still have requestDate (maybe in original_df_test), use it:\nif 'requestDate' in df_test.columns:\n    df_test['requestDate'] = pd.to_datetime(df_test['requestDate'], errors='coerce')\n\n    # Extract parts\n    df_test['requestDate_hour'] = df_test['requestDate'].dt.hour\n    df_test['requestDate_minute'] = df_test['requestDate'].dt.minute\n    df_test['requestDate_dayofweek'] = df_test['requestDate'].dt.dayofweek\n    df_test['requestDate_is_weekend'] = df_test['requestDate_dayofweek'].isin([5, 6]).astype(int)\n\n    # Sin-Cos encoding for hour (cyclical feature)\n    df_test['requestDate_hour_sin'] = np.sin(2 * np.pi * df_test['requestDate_hour'] / 24)\n    df_test['requestDate_hour_cos'] = np.cos(2 * np.pi * df_test['requestDate_hour'] / 24)\nelse:\n    print(\"âŒ requestDate column not found in test data. Cannot engineer time features.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:38:34.445379Z","iopub.execute_input":"2025-07-03T11:38:34.445714Z","iopub.status.idle":"2025-07-03T11:42:39.876844Z","shell.execute_reply.started":"2025-07-03T11:38:34.445685Z","shell.execute_reply":"2025-07-03T11:42:39.871108Z"}},"outputs":[{"name":"stdout","text":"Loaded 6897776 rows from 45231 sessions\nMin options per session: 1\nMax options per session: 7022\nStarting missing value imputation...\nOriginal missing values: 505902566\nâœ“ Filled corporateTariffCode with -1 (converted to int64)\nâœ“ Filled pricingInfo_isAccessTP with 0.0\nFilled frequentFlyer with NONE\nâœ“ Filled return flight timing fields with 'NONE'\nâœ“ Filled return flight timing fields with 'NONE'\nâœ“ Filled return flight timing fields with 'NONE'\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled 48 object segment columns with 'NONE'\nâœ“ Filled 32 float segment columns with 0.0\nâœ“ Filled cancellation and exchange rules with appropriate float values\n\n1. Converting datetime columns...\nProcessing legs0_arrivalAt...\nProcessing legs0_departureAt...\nProcessing legs1_arrivalAt...\nProcessing legs1_departureAt...\nDatetime conversion completed\n\n3. Optimizing categorical variables...\n   âœ“ frequentFlyer: 325 categories\n   âœ“ searchRoute: 3646 categories\n   âœ“ ranker_id: 45231 categories\n   âœ“ legs0_segments0_aircraft_code: 93 categories\n   âœ“ legs0_segments0_arrivalTo_airport_city_iata: 386 categories\n   âœ“ legs0_segments0_arrivalTo_airport_iata: 413 categories\n   âœ“ legs0_segments0_departureFrom_airport_iata: 290 categories\n   âœ“ legs0_segments0_flightNumber: 5714 categories\n   âœ“ legs0_segments0_marketingCarrier_code: 136 categories\n   âœ“ legs0_segments0_operatingCarrier_code: 170 categories\n   âœ“ legs0_segments1_aircraft_code: 91 categories\n   âœ“ legs0_segments1_arrivalTo_airport_city_iata: 322 categories\n   âœ“ legs0_segments1_arrivalTo_airport_iata: 355 categories\n   âœ“ legs0_segments1_departureFrom_airport_iata: 334 categories\n   âœ“ legs0_segments1_flightNumber: 4825 categories\n   âœ“ legs0_segments1_marketingCarrier_code: 129 categories\n   âœ“ legs0_segments1_operatingCarrier_code: 159 categories\n   âœ“ legs0_segments2_aircraft_code: 63 categories\n   âœ“ legs0_segments2_arrivalTo_airport_city_iata: 151 categories\n   âœ“ legs0_segments2_arrivalTo_airport_iata: 175 categories\n   âœ“ legs0_segments2_departureFrom_airport_iata: 184 categories\n   âœ“ legs0_segments2_flightNumber: 1564 categories\n   âœ“ legs0_segments2_marketingCarrier_code: 74 categories\n   âœ“ legs0_segments2_operatingCarrier_code: 96 categories\n   âœ“ legs1_segments0_aircraft_code: 92 categories\n   âœ“ legs1_segments0_arrivalTo_airport_city_iata: 243 categories\n   âœ“ legs1_segments0_arrivalTo_airport_iata: 258 categories\n   âœ“ legs1_segments0_departureFrom_airport_iata: 261 categories\n   âœ“ legs1_segments0_flightNumber: 3963 categories\n   âœ“ legs1_segments0_marketingCarrier_code: 120 categories\n   âœ“ legs1_segments0_operatingCarrier_code: 139 categories\n   âœ“ legs1_segments1_aircraft_code: 72 categories\n   âœ“ legs1_segments1_arrivalTo_airport_city_iata: 136 categories\n   âœ“ legs1_segments1_arrivalTo_airport_iata: 147 categories\n   âœ“ legs1_segments1_departureFrom_airport_iata: 216 categories\n   âœ“ legs1_segments1_flightNumber: 2205 categories\n   âœ“ legs1_segments1_marketingCarrier_code: 95 categories\n   âœ“ legs1_segments1_operatingCarrier_code: 101 categories\n   âœ“ legs1_segments2_aircraft_code: 34 categories\n   âœ“ legs1_segments2_arrivalTo_airport_city_iata: 34 categories\n   âœ“ legs1_segments2_arrivalTo_airport_iata: 39 categories\n   âœ“ legs1_segments2_departureFrom_airport_iata: 55 categories\n   âœ“ legs1_segments2_flightNumber: 179 categories\n   âœ“ legs1_segments2_marketingCarrier_code: 41 categories\n   âœ“ legs1_segments2_operatingCarrier_code: 44 categories\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"df_test_copy = df_test.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:42:39.878615Z","iopub.execute_input":"2025-07-03T11:42:39.878829Z","iopub.status.idle":"2025-07-03T11:43:03.533961Z","shell.execute_reply.started":"2025-07-03T11:42:39.878807Z","shell.execute_reply":"2025-07-03T11:43:03.528699Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# # 3. Load trained model\n# print(\"Loading model...\")\n# import joblib\n# model = joblib.load(\"/kaggle/input/lgbm/other/default/1/lgb_ranker_model1.pkl\")\n# expected_features = model.booster_.feature_name()\n# print(f\"Model expects {len(expected_features)} features\")\n\n# # Replace 'df_test' with your actual test DataFrame\n# current_features = set(df_test.columns) - {'Id', 'ranker_id'}  # Exclude ID columns\n# print(f\"Test data has {len(current_features)} features\")\n\n# # âœ… FIX: Convert both to sets before subtracting\n# missing_in_test = set(expected_features) - current_features\n# extra_in_test = current_features - set(expected_features)\n\n# print(f\"\\n=== FEATURE MISMATCH ANALYSIS ===\")\n# print(f\"Missing in test data ({len(missing_in_test)} features):\")\n# for feature in sorted(missing_in_test):\n#     print(f\"  - {feature}\")\n\n# print(f\"\\nExtra in test data ({len(extra_in_test)} features):\")\n# for feature in sorted(extra_in_test):\n#     print(f\"  - {feature}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===========================\n# Load Trained Model & Prepare Test Data\n# ===========================\n# Loads the previously saved LightGBM ranking model and prepares test data for inference.\n#\n# ðŸ”¹ Uses `joblib.load` to deserialize the trained model.\n# ðŸ”¹ Extracts expected feature names from the model to ensure exact feature alignment.\n# ðŸ”¹ Applies the same target encoder (`te`) to categorical columns as used during training.\n# ðŸ”¹ Drops non-feature columns (like `ranker_id`) and reorders test features to match the model.\n#\n# âœ… This ensures the test input schema is **identical** to what the model was trained on.\n# âš ï¸ Mismatched feature order or missing columns will cause inference to fail â€” always align!\n\nimport joblib\n# 3. Load trained model\nprint(\"Loading model...\")\nmodel = joblib.load(\"/kaggle/working/lgb_ranker_model3.pkl\")\nmodel_features = model.booster_.feature_name()\nprint(f\"Model expects {len(model_features)} features: {model_features[:5]}...\")\n\nX_test = df_test_copy.drop(columns=['ranker_id'], errors='ignore')\n\nX_test[cat_cols] = te.transform(X_test[cat_cols])\n\nX_test = X_test[model_features]\n#Ensure feature order matches\nX_test_ordered = X_test[model_features]\nprint(f\"Feature order aligned: {list(X_test_ordered.columns) == list(model_features)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:43:03.535746Z","iopub.execute_input":"2025-07-03T11:43:03.535978Z","iopub.status.idle":"2025-07-03T11:46:04.919847Z","shell.execute_reply.started":"2025-07-03T11:43:03.535956Z","shell.execute_reply":"2025-07-03T11:46:04.914830Z"}},"outputs":[{"name":"stdout","text":"Loading model...\nModel expects 130 features: ['Id', 'bySelf', 'companyID', 'corporateTariffCode', 'frequentFlyer']...\nFeature order aligned: True\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ===========================\n# Prediction & Submission Generation\n# ===========================\n# Uses the trained LightGBM ranker model to generate predictions on the test dataset\n# and prepares the final submission file in the required format.\n#\n# ðŸ”¹ `model.predict(X_test)`: Generates ranking scores for each flight option.\n# ðŸ”¹ Constructs a submission DataFrame with:\n#     - `Id`: Unique identifier for each flight option\n#     - `ranker_id`: Session group identifier\n#     - `selected`: Rank of the option within each session (1 = highest score)\n#\n# âš ï¸ We use `groupby('ranker_id')` and `rank(..., ascending=False)` to ensure the highest score is ranked first.\n# ðŸ”¹ Prediction scores (`pred`) are dropped from the final CSV â€” only `Id`, `ranker_id`, and `selected` are kept.\n# ðŸ”¹ Submission row order is aligned with original test data for consistency.\n#\n\n\n# Predict using your trained LightGBM ranker model\nprint(\"Predicting....\")\ntest_preds = model.predict(X_test)  # Make sure X_test matches training features\nprint(\"Successfully Predicted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:46:04.922571Z","iopub.execute_input":"2025-07-03T11:46:04.922840Z","iopub.status.idle":"2025-07-03T11:46:21.528826Z","shell.execute_reply.started":"2025-07-03T11:46:04.922814Z","shell.execute_reply":"2025-07-03T11:46:21.524021Z"}},"outputs":[{"name":"stdout","text":"Predicting....\nSuccessfully Predicted!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"X_test.shape\nlen(model_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T10:13:29.377343Z","iopub.execute_input":"2025-07-03T10:13:29.377699Z","iopub.status.idle":"2025-07-03T10:13:29.392774Z","shell.execute_reply.started":"2025-07-03T10:13:29.377671Z","shell.execute_reply":"2025-07-03T10:13:29.387505Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"130"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# Assuming your test DataFrame is called test_df and has 'Id' and 'ranker_id' columns\nsubmission_df = df_test_copy[['Id', 'ranker_id']].copy()\nsubmission_df['pred'] = test_preds\n\n# Rank within each session (ranker_id); higher score = better â†’ rank 1 is best\nsubmission_df['selected'] = submission_df.groupby('ranker_id')['pred'] \\\n    .rank(method='first', ascending=False).astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:46:21.530305Z","iopub.execute_input":"2025-07-03T11:46:21.530508Z","iopub.status.idle":"2025-07-03T11:46:24.054910Z","shell.execute_reply.started":"2025-07-03T11:46:21.530488Z","shell.execute_reply":"2025-07-03T11:46:24.049527Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Drop prediction scores (not needed in submission)\nsubmission_df = submission_df.drop(columns=['pred'])\n\n# Ensure row order is exactly like test_df\nsubmission_df = submission_df.set_index('Id').loc[df_test['Id']].reset_index()\n\nsubmission_df.head()\n# Save as CSV\nsubmission_df.to_csv('submission1.csv', index=False)\nprint(\"ðŸ“¦ Submission saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T11:46:24.057080Z","iopub.execute_input":"2025-07-03T11:46:24.057304Z","iopub.status.idle":"2025-07-03T11:46:37.811551Z","shell.execute_reply.started":"2025-07-03T11:46:24.057283Z","shell.execute_reply":"2025-07-03T11:46:37.807183Z"}},"outputs":[{"name":"stdout","text":"ðŸ“¦ Submission saved as submission.csv\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Current Leaderboard Status ðŸŽ¯\n\nWeâ€™ve achieved a **score of 0.31126** on the leaderboard so far â€” a solid baseline!\n\nðŸš€ Next steps: Focus on **feature engineering** and model tuning to push this score higher.  \nStay tuned for updates as we iterate and refine the pipeline.  ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}